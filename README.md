# VÃ­tor GonÃ§alves - Local LLM Chatbot

## Status
**ðŸš§ Proof of Concept - Active Development**

A fully client-side chatbot running WebLLM (Phi-3.5-mini) entirely in the browser using WebGPU.

## Goal
Demonstrate the viability of running LLMs locally in the browser with complete privacy - zero server communication, all processing happens client-side.

## Requirements

### Technical
- **Browser:** Chrome 113+ or Edge 113+ (WebGPU required)
- **Server:** Must be served via HTTP (not `file://` due to ES modules)
- **Resources:** ~2-4GB RAM, 1.9GB storage for cached model

### Design Constraints
- Mobile-only interface (375px Ã— 620px)
- No external API calls
- No conversation persistence

## Quick Start
```bash
python -m http.server 8000
# Open http://localhost:8000
```

First load will download the model (~1.9GB). Subsequent loads are instant.

## Technology
- **LLM:** Phi-3.5-mini via WebLLM (MLC AI)
- **Acceleration:** WebGPU
- **Fonts:** Young Serif, Work Sans

## How It Works

### Query Flow
1. **User Input** â†’ User sends a message through the chat interface
2. **Context Extraction** â†’ Previous conversation messages are limited to last 10 turns to manage memory
3. **User Info Injection** â†’ Extracted user details (name, email, company, context) from earlier messages are added to system prompt for persistent context
4. **RAG Augmentation** _(Future)_ â†’ Semantic search against `embeddings.json` to find relevant portfolio data chunks
5. **Model Query** â†’ Conversation history + system instructions sent to local LLM via WebLLM
6. **Streaming Response** â†’ Model generates response with streaming enabled for better UX
7. **Metadata Extraction** â†’ Model appends `[EXTRACT]{...}[/EXTRACT]` JSON with user info (stripped before display)
8. **Response Display** â†’ Clean response shown to user, extracted info stored for future context

### Context Management
- **System Instructions**: Define model personality (Goma), behavior guidelines, and extraction format
- **Conversation History**: Rolling window of last 10 messages to prevent memory bloat
- **User Context**: Name, email, company, position, and interests persist across entire session
- **Extraction Format**: Model embeds structured JSON in every response for zero-overhead data capture

### RAG System _(In Development)_
- **Embeddings**: 19 pre-generated chunks (summary, skills, education, experience, projects) with 384-dim vectors
- **Model**: `all-MiniLM-L6-v2` via sentence-transformers
- **Search**: Client-side cosine similarity to find top-k relevant chunks
- **Injection**: Relevant context prepended to system prompt before model query

Generate embeddings:
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install sentence-transformers
python generate_embeddings.py
```

### Data Structure: embeddings.json

The `embeddings.json` file serves dual purposes:
1. **LLM Context** - Provides relevant portfolio information to the chatbot during conversations
2. **Vector Search** - Powers the static portfolio search functionality using semantic similarity

**Structure**:
```json
[
  {
    "text": "Chunk of text content (summary, skill, project description, etc.)",
    "embedding": [0.123, -0.456, ...],  // 384-dimensional vector from all-MiniLM-L6-v2
    "project": "Project Name",           // Groups results in search (optional)
    "section": "Section Name",           // Alternative grouping (optional)
    "anchor": "element-id",              // HTML element ID to scroll to (optional)
    "image": "path/to/image.jpg"         // Associated image for visual display (optional)
  }
]
```

**Field Guidelines**:
- **text** (required): The actual content chunk. Keep chunks focused and self-contained (1-3 sentences ideal).
- **embedding** (required): Generated by `generate_embeddings.py`. 384-dimensional vector for semantic matching.
- **project/section** (recommended): Used to group related results in search UI. Use `project` for project-specific chunks, `section` for general info (Skills, Experience, Education).
- **anchor** (optional): HTML element ID for smooth scrolling from search results to specific content.
- **image** (optional): Path to image associated with this chunk. Useful for project screenshots, skill icons, etc.

**Chunking Strategy**:
- **Profile/Summary**: 1-2 chunks covering who you are and what you do
- **Skills**: 1 chunk per skill category (Technical, Design, Languages)
- **Experience**: 1-2 chunks per job/role highlighting key achievements
- **Education**: 1 chunk per degree/certification
- **Projects**: 2-4 chunks per project covering different aspects:
  - Overview (problem statement, your role)
  - Key features or methodologies
  - Technologies used
  - Results/impact

**Example Chunk**:
```json
{
  "text": "Designed and implemented a mobile-first UX system for a fintech startup, resulting in 40% increase in user engagement and 25% reduction in support tickets.",
  "embedding": [...],
  "project": "Fintech Mobile App",
  "anchor": "project-fintech",
  "image": "res/img/fintech-dashboard.png"
}
```

**Regenerating Embeddings**:
When you update `data.json` with new chunks, regenerate embeddings:
```bash
.\venv\Scripts\Activate.ps1
python generate_embeddings.py
```

This creates a new `embeddings.json` with updated vector representations.

## Features
- 100% local AI processing
- Performance monitoring with warnings
- AI on/off toggle
- Accessibility mode
- Cache management

## Limitations
- WebGPU support limited (Chrome/Edge only)
- Large initial download (1.9GB)
- No conversation history persistence

## Development
See `logs/` folder for detailed development notes, abandoned ideas, and changelog.

## Credits
- **Built by:** VÃ­tor GonÃ§alves
- **AI Assistant:** Claude (Anthropic) - Architecture & implementation support
- **WebLLM:** MLC AI Project
- **Model:** Phi-3.5-mini (Microsoft)
- **Fonts:** Google Fonts
